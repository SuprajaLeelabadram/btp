# -*- coding: utf-8 -*-
"""BTP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11I7g10ZMd-oXM1nqPaJC4keFMuetgsNg
"""

# Commented out IPython magic to ensure Python compatibility.
import matplotlib.pyplot as plt
import cv2
import os, glob
import numpy as np

# %matplotlib inline
# %config InlineBackend.figure_format = 'retina'

def show_images(images, cmap=None):
    cols = 1
    rows = (len(images)+1)//cols
    
    plt.figure(figsize=(30, 35))
    for i, image in enumerate(images):
        plt.subplot(rows, cols, i+1)
        cmap = 'gray' if len(image.shape)==2 else cmap
        plt.imshow(image, cmap=cmap)

    plt.tight_layout(pad=0, h_pad=0, w_pad=0)
    plt.show()
    

test_images = [plt.imread(path) for path in glob.glob(r'/content/drive/MyDrive/supraja_btp/Dataset/val/images/*.jpg')]
show_images(test_images)

def filter_region(image, vertices):
    mask = np.zeros_like(image)
    if len(mask.shape)==2:
        cv2.fillPoly(mask, vertices, 255)
    else:
        cv2.fillPoly(mask, vertices, (255,)*mask.shape[2])       
    return cv2.bitwise_and(image, mask)

def select_region(image):
    rows, cols = image.shape[:2]
    bottom_left  = [cols*0.32, rows*1]
    top_left     = [cols*0.35, rows*0.0]
    bottom_right = [cols*0.68, rows*1]
    top_right    = [cols*0.56, rows*0.0] 
    vertices = np.array([[bottom_left, top_left, top_right, bottom_right]], dtype=np.int32)
    return filter_region(image, vertices)

roi_images = list(map(select_region,test_images))

show_images(roi_images)

# Commented out IPython magic to ensure Python compatibility.
!git clone https://github.com/matterport/Mask_RCNN.git
# %cd Mask_RCNN
!python setup.py install

!pip uninstall -y tensorflow
!pip install tensorflow-gpu==1.15 
!pip install Keras==2.1.0
!pip uninstall h5py
!pip install h5py==2.10.0

import cv2
import tensorflow as tf
import keras
from os import listdir
from xml.etree import ElementTree
from numpy import zeros
from numpy import asarray
from mrcnn.utils import Dataset
from matplotlib import pyplot
from mrcnn.visualize import display_instances
from mrcnn.utils import extract_bboxes
from mrcnn.config import Config
from mrcnn.model import load_image_gt
from mrcnn.model import MaskRCNN
from tensorflow.keras import backend
from mrcnn.model import mold_image
from numpy import expand_dims
from numpy import mean
from mrcnn.utils import compute_ap
from numpy import expand_dims
from matplotlib.patches import Rectangle

class BuildDataset(Dataset):

	# load the dataset definitions
	def load_dataset(self, dataset_dir, is_train=True):
	
		# define one class
		self.add_class("dataset", 1, "encroachment")
		
	
		# define data locations
		images_dir = dataset_dir + '/'

		for filename in listdir(images_dir):

			# extract image id
			
			image_id = filename[:-4]            
			# image_id_1 = filename[5:-4]
     
			img_path = images_dir + filename

			self.add_image('dataset', image_id=image_id, path=img_path,class_ids=[0,1])

class BuildDataset(Dataset):
  # load the dataset definitions
	def load_dataset(self, dataset_dir, is_train=True):
		# define one class
		self.add_class("dataset", 1, "encroachment")
		# define data locations
		images_dir = dataset_dir + '/images/'
		annotations_dir = dataset_dir + '/anotations/'
		# find all images
		for filename in listdir(images_dir):
     	# extract image id
			image_id = filename[:-4]            
			image_id_1 = filename[5:-4]
      img_path = images_dir + filename
			ann_path = annotations_dir + image_id + '.xml'
			# add to dataset
			self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path,class_ids=[0,1])

	# extract bounding boxes from an annotation file
	def extract_boxes(self, filename):
		# load and parse the file
		tree = ElementTree.parse(filename)
		# get the root of the document
		root = tree.getroot()
		# extract each bounding box
		boxes = list()
		for box in root.findall('.//object'):
			name = box.find('name').text
			xmin = int(box.find('./bndbox/xmin').text)
			ymin = int(box.find('./bndbox/ymin').text)
			xmax = int(box.find('./bndbox/xmax').text)
			ymax = int(box.find('./bndbox/ymax').text)
			coors = [xmin, ymin, xmax, ymax, name]
			boxes.append(coors)
		# extract image dimensions
		width = int(root.find('.//size/width').text)
		height = int(root.find('.//size/height').text)
		return boxes, width, height

	def load_mask(self, image_id):
		info = self.image_info[image_id]
		# define box file location
		path = info['annotation']
		# load XML
		boxes, w, h = self.extract_boxes(path)
		# create one array for all masks, each on a different channel
		masks = zeros([h, w, len(boxes)], dtype='uint8')
		# create masks
		class_ids = list()
		for i in range(len(boxes)):
			box = boxes[i]
			box[4] = box[4].upper()
			row_s, row_e = box[1], box[3]
			col_s, col_e = box[0], box[2]
			masks[row_s:row_e, col_s:col_e, i] = 1
			class_ids.append(self.class_names.index('encroachment'))
		return masks, asarray(class_ids, dtype='int32')

	# load an image reference
	def image_reference(self, image_id):
		info = self.image_info[image_id]
		return info['path']

	def custom_test(self, dataset_dir,image_ids):
	
		# define one class
		self.add_class("dataset", 1, "encroachment")
		# define data locations
		images_dir = dataset_dir + '/images/'
		annotations_dir = dataset_dir + '/anotations/'
		
		for i in image_ids :
				# find all images
				filename = 'frame' + str(i) +'.jpg'
				img_path = images_dir + filename
				ann_path = annotations_dir + 'frame' + str(i) + '.xml'
				# add to dataset
				self.add_image('dataset', image_id= str(i), path=img_path, annotation=ann_path,class_ids=[0,1])

# train set
train_set = BuildDataset()
train_set.load_dataset('/content/drive/MyDrive/supraja_btp/Dataset/train/', is_train=True)
train_set.prepare()
print('Train: %d' % len(train_set.image_ids))

# test/val set
test_set = BuildDataset()
test_set.load_dataset('/content/drive/MyDrive/supraja_btp/Dataset/val/', is_train=False)
test_set.prepare()
print('Test: %d' % len(test_set.image_ids))

image_id = 6
# load the image
image = train_set.load_image(image_id)
# load the masks and the class ids
mask, class_ids = train_set.load_mask(image_id)
# extract bounding boxes from the masks
print(class_ids)
print(train_set.class_names)
bbox = extract_bboxes(mask)
# display image with masks and bounding boxes
display_instances(image, bbox, mask, class_ids, train_set.class_names)

# define a configuration for the model
class BuildConfig(Config):
	# define the name of the configuration
	NAME = "Build_cfg"
	# number of classes (background + encroachment)
	NUM_CLASSES = 1 + 1 
	# number of training steps per epoch
	STEPS_PER_EPOCH = 10
	DETECTION_MIN_CONFIDENCE = 0.9

# prepare config
config = BuildConfig()
config.display()
# define the model
model = MaskRCNN(mode='training', model_dir='./', config=config)
# load weights (mscoco) and exclude the output layers
model.load_weights('/content/drive/MyDrive/supraja_btp/mask_rcnn_re_coco.h5', by_name=True, exclude=["mrcnn_class_logits", "mrcnn_bbox_fc",  "mrcnn_bbox", "mrcnn_mask"])
model.train(train_set, test_set, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')

# define the prediction configuration
class PredictionConfig(Config):
	# define the name of the configuration
	NAME = "Build_cfg"
	# number of classes (background + encroachment)
	NUM_CLASSES = 1 + 1 
	# simplify GPU config
	GPU_COUNT = 1
	IMAGES_PER_GPU = 1

# calculate the mAP for a model on a given dataset
def evaluate_model(dataset, model, cfg):
	APs = list()
	for image_id in dataset.image_ids:
		# load image, bounding boxes and masks for the image id
		image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(dataset, cfg, image_id, use_mini_mask=False)
		# convert pixel values (e.g. center)
		scaled_image = mold_image(image, cfg)
		# convert image into one sample
		sample = expand_dims(scaled_image, 0)
		# make prediction
		yhat = model.detect(sample, verbose=0)
		# extract results for first sample
		r = yhat[0]
		# calculate statistics, including AP
		AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r["rois"], r["class_ids"], r["scores"], r['masks'])
		# store
		APs.append(AP)
	# calculate the mean AP across all images
	mAP = mean(APs)
	return mAP

cfg = PredictionConfig()
# define the model
mode = MaskRCNN(mode='inference', model_dir='./', config=cfg)
# load model weights
mode.load_weights('/content/drive/MyDrive/supraja_btp/mask_rcnn_re_coco.h5', by_name=True)
#evaluate model on training dataset
train_mAP = evaluate_model(train_set, mode, cfg)
print("Train mAP: %.3f" % train_mAP)



# plot a number of photos with ground truth and predictions
def plot_actual_vs_predicted(dataset, model, cfg, n_images=5, is_true = True):
    arr = ['BG','encroachment']
    # load image and mask
    for i in range(n_images):
      # load the image and mask
      image = dataset.load_image(i)
      #if image is present in the dataset
      if is_true :
        mask, class_ids = dataset.load_mask(i)
      # convert pixel values (e.g. center)
      scaled_image = mold_image(image, cfg)
      # convert image into one sample
      sample = expand_dims(scaled_image, 0)
      # make prediction
      yhat = model.detect(sample, verbose=0)[0]
      # define subplot
      pyplot.subplot(n_images, 2, i*2+1)
      # plot raw pixel data
      pyplot.imshow(image)
      pyplot.title('Actual')
      # plot masks
      if is_true :
        for j in range(mask.shape[2]):
            pyplot.imshow(mask[:, :, j], cmap='gray', alpha=0.3)
      # get the context for drawing boxes
      pyplot.subplot(n_images, 2, i*2+2)
      # plot raw pixel data
      pyplot.imshow(image)
      pyplot.title('Predicted')
      if is_true :
        pyplot.title('class : ' + arr[class_ids[0]])
      ax = pyplot.gca()
      # plot each box
      for box in yhat['rois']:
        # get coordinates
        y1, x1, y2, x2 = box
        # calculate width and height of the box
        width, height = x2 - x1, y2 - y1
        # create the shape
        rect = Rectangle((x1, y1), width, height, fill=False, color='red')
        # draw the box
        ax.add_patch(rect)
    # show the figure
    pyplot.show()